{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e77bb25",
   "metadata": {},
   "source": [
    "Question 1: OLS Regression â€“ Growth [25 points]\n",
    "Using the Growth data, carry out the following exercises. Exclude the data for Malta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d7c7a9",
   "metadata": {},
   "source": [
    "a. [5] Construct a table that shows the sample mean, standard deviation,\n",
    "and minimum and maximum values for the series Growth, TradeShare, YearsSchool, Oil, Rev_Coups, Assassinations, and RGDP60. Include the appropriate units for all entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96bd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data, excluding Malta\n",
    "df = pd.read_stata('data/Growth.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ca7583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_name</th>\n",
       "      <th>growth</th>\n",
       "      <th>oil</th>\n",
       "      <th>rgdp60</th>\n",
       "      <th>tradeshare</th>\n",
       "      <th>yearsschool</th>\n",
       "      <th>rev_coups</th>\n",
       "      <th>assasinations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India</td>\n",
       "      <td>1.915168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>765.999817</td>\n",
       "      <td>0.140502</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>0.617645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4462.001465</td>\n",
       "      <td>0.156623</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japan</td>\n",
       "      <td>4.304759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2953.999512</td>\n",
       "      <td>0.157703</td>\n",
       "      <td>6.71</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>2.930097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1783.999878</td>\n",
       "      <td>0.160405</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "      <td>1.712265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9895.003906</td>\n",
       "      <td>0.160815</td>\n",
       "      <td>8.66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Cyprus</td>\n",
       "      <td>5.384184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2037.000366</td>\n",
       "      <td>0.979355</td>\n",
       "      <td>4.29</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>4.114544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1420.000244</td>\n",
       "      <td>1.105364</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>2.651335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5495.001953</td>\n",
       "      <td>1.115917</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Mauritius</td>\n",
       "      <td>3.024178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2861.999268</td>\n",
       "      <td>1.127937</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Malta</td>\n",
       "      <td>6.652838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1374.000000</td>\n",
       "      <td>1.992616</td>\n",
       "      <td>5.64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country_name    growth  oil       rgdp60  tradeshare  yearsschool  \\\n",
       "0           India  1.915168  0.0   765.999817    0.140502         1.45   \n",
       "1       Argentina  0.617645  0.0  4462.001465    0.156623         4.99   \n",
       "2           Japan  4.304759  0.0  2953.999512    0.157703         6.71   \n",
       "3          Brazil  2.930097  0.0  1783.999878    0.160405         2.89   \n",
       "4   United States  1.712265  0.0  9895.003906    0.160815         8.66   \n",
       "..            ...       ...  ...          ...         ...          ...   \n",
       "60         Cyprus  5.384184  0.0  2037.000366    0.979355         4.29   \n",
       "61       Malaysia  4.114544  0.0  1420.000244    1.105364         2.34   \n",
       "62        Belgium  2.651335  0.0  5495.001953    1.115917         7.46   \n",
       "63      Mauritius  3.024178  0.0  2861.999268    1.127937         2.44   \n",
       "64          Malta  6.652838  0.0  1374.000000    1.992616         5.64   \n",
       "\n",
       "    rev_coups  assasinations  \n",
       "0    0.133333       0.866667  \n",
       "1    0.933333       1.933333  \n",
       "2    0.000000       0.200000  \n",
       "3    0.100000       0.100000  \n",
       "4    0.000000       0.433333  \n",
       "..        ...            ...  \n",
       "60   0.100000       0.166667  \n",
       "61   0.033333       0.033333  \n",
       "62   0.000000       0.000000  \n",
       "63   0.000000       0.000000  \n",
       "64   0.000000       0.000000  \n",
       "\n",
       "[65 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d349549d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>growth</th>\n",
       "      <td>1.869120</td>\n",
       "      <td>1.816189</td>\n",
       "      <td>-2.811944</td>\n",
       "      <td>7.156855</td>\n",
       "      <td>percent per year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tradeshare</th>\n",
       "      <td>0.542392</td>\n",
       "      <td>0.228333</td>\n",
       "      <td>0.140502</td>\n",
       "      <td>1.127937</td>\n",
       "      <td>fraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearsschool</th>\n",
       "      <td>3.959219</td>\n",
       "      <td>2.553465</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.070000</td>\n",
       "      <td>years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>indicator (1/0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_coups</th>\n",
       "      <td>0.170067</td>\n",
       "      <td>0.225456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assasinations</th>\n",
       "      <td>0.281901</td>\n",
       "      <td>0.494159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.466667</td>\n",
       "      <td>number per million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rgdp60</th>\n",
       "      <td>3130.812500</td>\n",
       "      <td>2522.978516</td>\n",
       "      <td>366.999939</td>\n",
       "      <td>9895.003906</td>\n",
       "      <td>US dollars (1960)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean          std         min          max  \\\n",
       "growth            1.869120     1.816189   -2.811944     7.156855   \n",
       "tradeshare        0.542392     0.228333    0.140502     1.127937   \n",
       "yearsschool       3.959219     2.553465    0.200000    10.070000   \n",
       "oil               0.000000     0.000000    0.000000     0.000000   \n",
       "rev_coups         0.170067     0.225456    0.000000     0.970370   \n",
       "assasinations     0.281901     0.494159    0.000000     2.466667   \n",
       "rgdp60         3130.812500  2522.978516  366.999939  9895.003906   \n",
       "\n",
       "                            units  \n",
       "growth           percent per year  \n",
       "tradeshare               fraction  \n",
       "yearsschool                 years  \n",
       "oil               indicator (1/0)  \n",
       "rev_coups                  number  \n",
       "assasinations  number per million  \n",
       "rgdp60          US dollars (1960)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_nomalta = df[df['country_name'] != 'Malta']\n",
    "\n",
    "# Correct column names to match the DataFrame\n",
    "cols = ['growth', 'tradeshare', 'yearsschool', 'oil', 'rev_coups', 'assasinations', 'rgdp60']\n",
    "\n",
    "# Compute summary statistics\n",
    "summary = df_nomalta[cols].agg(['mean', 'std', 'min', 'max']).T\n",
    "\n",
    "# Optionally add units for interpretation (as a column or inline comments)\n",
    "units = {\n",
    "    'growth': 'percent per year',\n",
    "    'tradeshare': 'fraction',\n",
    "    'yearsschool': 'years',\n",
    "    'oil': 'indicator (1/0)',\n",
    "    'rev_coups': 'number',\n",
    "    'assasinations': 'number per million',\n",
    "    'rgdp60': 'US dollars (1960)'\n",
    "}\n",
    "\n",
    "summary['units'] = summary.index.map(units)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef63fdb",
   "metadata": {},
   "source": [
    "b. Run a regression of Growth on TradeShare, YearsSchool, Rev_Coups, Assassinations,\n",
    "and RGDP60. What is the value of the coefficient on Rev_Coups? Interpret the value of\n",
    "this coefficient. Is it large or small in a real-world sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65fef36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 growth   R-squared:                       0.291\n",
      "Model:                            OLS   Adj. R-squared:                  0.230\n",
      "Method:                 Least Squares   F-statistic:                     4.764\n",
      "Date:                Wed, 12 Nov 2025   Prob (F-statistic):            0.00103\n",
      "Time:                        19:43:04   Log-Likelihood:                -117.49\n",
      "No. Observations:                  64   AIC:                             247.0\n",
      "Df Residuals:                      58   BIC:                             259.9\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "const             0.6269      0.783      0.801      0.427      -0.941       2.194\n",
      "tradeshare        1.3408      0.960      1.397      0.168      -0.581       3.263\n",
      "yearsschool       0.5642      0.143      3.943      0.000       0.278       0.851\n",
      "rev_coups        -2.1504      1.119     -1.922      0.059      -4.390       0.089\n",
      "assasinations     0.3226      0.488      0.661      0.511      -0.654       1.299\n",
      "rgdp60           -0.0005      0.000     -3.059      0.003      -0.001      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                        7.569   Durbin-Watson:                   2.130\n",
      "Prob(Omnibus):                  0.023   Jarque-Bera (JB):                8.597\n",
      "Skew:                           0.494   Prob(JB):                       0.0136\n",
      "Kurtosis:                       4.499   Cond. No.                     2.59e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.59e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "Coefficient on rev_coups: -2.1504\n",
      "\n",
      "Rev_Coups ë³€ìˆ˜ì˜ ê³„ìˆ˜ëŠ” -2.1504ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì´ ê³ ì •ëœ ìƒíƒœì—ì„œ, ì •ë³€ ì¿ ë°íƒ€(Rev_Coups)ê°€ 1 ì¦ê°€í•  ë•Œ ê²½ì œ ì„±ì¥ë¥ (growth)ì´ í‰ê· ì ìœ¼ë¡œ -2.1504 í¬ì¸íŠ¸ ë³€í™”í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ê³„ìˆ˜ëŠ” ì‹¤ì§ˆì ìœ¼ë¡œ ë§¤ìš° í° ê°’ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# ë…ë¦½ ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ì¤€ë¹„\n",
    "X = df_nomalta[['tradeshare', 'yearsschool', 'rev_coups', 'assasinations', 'rgdp60']]\n",
    "X = sm.add_constant(X)\n",
    "y = df_nomalta['growth']\n",
    "\n",
    "# íšŒê·€ë¶„ì„ ì‹¤í–‰\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(model.summary())\n",
    "\n",
    "# rev_coups ê³„ìˆ˜ ì¶”ì¶œ ë° ì¶œë ¥\n",
    "rev_coups_coef = model.params['rev_coups']\n",
    "print(f\"\\nCoefficient on rev_coups: {rev_coups_coef:.4f}\")\n",
    "\n",
    "# í•´ì„ì„ í•œêµ­ì–´ë¡œ ì¶œë ¥\n",
    "if abs(rev_coups_coef) >= 0.5:\n",
    "    size_comment = \"ì‹¤ì§ˆì ìœ¼ë¡œ ë§¤ìš° í° ê°’ì…ë‹ˆë‹¤.\"\n",
    "elif abs(rev_coups_coef) >= 0.1:\n",
    "    size_comment = \"ì‹¤ì§ˆì ìœ¼ë¡œ ì¤‘ê°„ ì •ë„ì˜ ê°’ì…ë‹ˆë‹¤.\"\n",
    "else:\n",
    "    size_comment = \"ì‹¤ì§ˆì ìœ¼ë¡œ ì‘ì€ ê°’ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "ko_message = (\n",
    "    f\"Rev_Coups ë³€ìˆ˜ì˜ ê³„ìˆ˜ëŠ” {rev_coups_coef:.4f}ì…ë‹ˆë‹¤. \"\n",
    "    \"ì´ëŠ” ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì´ ê³ ì •ëœ ìƒíƒœì—ì„œ, ì •ë³€ ì¿ ë°íƒ€(Rev_Coups)ê°€ 1 ì¦ê°€í•  ë•Œ \"\n",
    "    f\"ê²½ì œ ì„±ì¥ë¥ (growth)ì´ í‰ê· ì ìœ¼ë¡œ {rev_coups_coef:.4f} í¬ì¸íŠ¸ ë³€í™”í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. \"\n",
    "    f\"ì´ ê³„ìˆ˜ëŠ” {size_comment}\"\n",
    ")\n",
    "print(\"\\n\"+ko_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eae8a6",
   "metadata": {},
   "source": [
    "c. Use the regression to predict the average annual growth rate for a country that has\n",
    "average values for all regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96550db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted average annual growth rate (using regressor means): 1.8691\n"
     ]
    }
   ],
   "source": [
    "# í‰ê· ê°’ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ìƒˆë¡œìš´ ê´€ì¸¡ì¹˜ë¥¼ ìƒì„±\n",
    "mean_values = df_nomalta[['tradeshare', 'yearsschool', 'rev_coups', 'assasinations', 'rgdp60']].mean()\n",
    "mean_values_with_const = [1] + mean_values.tolist()  # const í¬í•¨\n",
    "\n",
    "# ì˜ˆì¸¡ (í‰ê· ê°’ ëŒ€ì…)\n",
    "pred_growth = model.predict([mean_values_with_const])[0]\n",
    "print(f\"Predicted average annual growth rate (using regressor means): {pred_growth:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb42bd8",
   "metadata": {},
   "source": [
    "d. Repeat (c), but now assume that the countryâ€™s value for TradeShare is one standard\n",
    "deviation above the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94ecbc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted average annual growth rate (TradeShare = mean + 1 std): 2.1753\n"
     ]
    }
   ],
   "source": [
    "# í‰ê· ê°’ ë²¡í„°ë¥¼ ë³µì‚¬í•˜ì—¬ TradeShareë§Œ í•œ í‘œì¤€í¸ì°¨ ë”í•œ ê°’ìœ¼ë¡œ ë³€ê²½\n",
    "mean_vals_sd = mean_values.copy()\n",
    "trade_share_mean = mean_values['tradeshare']\n",
    "trade_share_std = df_nomalta['tradeshare'].std()\n",
    "mean_vals_sd['tradeshare'] = trade_share_mean + trade_share_std\n",
    "\n",
    "mean_vals_sd_with_const = [1] + mean_vals_sd.tolist()  # const í¬í•¨\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "pred_growth_sd = model.predict([mean_vals_sd_with_const])[0]\n",
    "print(f\"Predicted average annual growth rate (TradeShare = mean + 1 sigma): {pred_growth_sd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186fe077",
   "metadata": {},
   "source": [
    "e. Why is Oil omitted from the regression? What would happen if it were included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f41cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oil ë³€ìˆ˜ëŠ” ë°ì´í„°ì…‹ì—ì„œ ëª¨ë‘ 0ì˜ ê°’ì„ ê°€ì§€ë¯€ë¡œ, ë¶„ì‚°ì´ 0ì´ê³  ìƒìˆ˜í•­ê³¼ ì™„ì „íˆ ê³µì„ (linearly dependent)í•©ë‹ˆë‹¤.\n",
    "# ì´ëŸ° ê²½ìš° íšŒê·€ì‹ì— í¬í•¨ì‹œí‚¤ë©´ ì™„ë²½í•œ ë‹¤ì¤‘ê³µì„ ì„±(perfect multicollinearity)ë¡œ ì¸í•´ statsmodelsê°€ ìë™ìœ¼ë¡œ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì œì™¸(omit)í•©ë‹ˆë‹¤.\n",
    "# ë§Œì•½ ê°•ì œë¡œ í¬í•¨í•˜ë ¤ í•œë‹¤ë©´ íšŒê·€ëª¨í˜•ì´ ì¶”ì •ë˜ì§€ ì•Šê±°ë‚˜, ê²°ê³¼ì—ì„œ ìë™ìœ¼ë¡œ ì œê±°ë˜ì–´ ë‚˜íƒ€ë‚  ê²ƒì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cef10e",
   "metadata": {},
   "source": [
    "Question 2: Instrumental Variables â€“ Returns to Schooling [40 points]\n",
    "In this exercise, you will follow Card (1993) to estimate the returns to schooling, which is\n",
    "a classic question in labor economics. It is generally accepted that schooling is positively\n",
    "associated with earnings, but measuring the causal relationship between them is not\n",
    "straightforward due to the endogeneity in schooling. To obtain a plausibly exogenous variation in\n",
    "schooling, Card (1993) uses geographic proximity to colleges as an instrument for schooling. Use\n",
    "the stata file, Card.dta.\n",
    "To be more concrete, consider the following causal model:\n",
    "log ğ‘Œ = ğ›¼ + ğ›¿ğ‘  + ğ›¾ğ‘‹ + ğœ€\n",
    "where ğ‘Œ is earnings, ğ‘  is years of schooling, ğ‘‹ is an (exogenous) covariate matrix, and ğœ€ is an\n",
    "individual-level idiosyncratic shock. The OLS estimate of ğ›¿ will not be consistent for the causal\n",
    "effect because ğ‘  is endogenous. For example, ğœ€ may include unobserved characteristics, such as\n",
    "an individualâ€™s ability, which may correlate with both ğ‘  and ğ‘Œ. Thus, Card (1993) proposes using\n",
    "ğ‘§, an indicator for having a 4-year college in oneâ€™s county, as an instrumental variable for ğ‘ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17161240",
   "metadata": {},
   "source": [
    "a) [10] In order for ğ‘§ to be a legitimate instrument variable, it must be a strong predictor of ğ‘ .\n",
    "Conceptually, do you think living close to a 4-year college in oneâ€™s county is a good predictor\n",
    "of oneâ€™s schooling? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5674baca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Living close to a 4-year college is likely a good predictor of one's years of schooling. Proximity lowers costs and increases accessibility, making attending college more feasible, particularly for students from families with financial constraints. However, it is not a perfect predictor because other factors (like personal motivation, academic ability, or family expectations) also affect schooling.\n"
     ]
    }
   ],
   "source": [
    "# Conceptually assessing whether living close to a 4-year college is a good predictor of years of schooling\n",
    "\n",
    "# In general, proximity to a 4-year college may increase the likelihood that an individual pursues further education\n",
    "# after high school due to reduced costs (such as moving or commuting) and increased accessibility. \n",
    "# Especially for students from lower-income families, local college availability often plays an important role \n",
    "# in the decision to continue schooling. However, other factors such as family preferences, individual ability, \n",
    "# and the value placed on higher education can also influence educational attainment, meaning proximity is not \n",
    "# a perfect predictor, but it is plausibly correlated with schooling.\n",
    "\n",
    "conceptual_iv_predictor = (\n",
    "    \"Living close to a 4-year college is likely a good predictor of one's years of schooling. \"\n",
    "    \"Proximity lowers costs and increases accessibility, making attending college more feasible, \"\n",
    "    \"particularly for students from families with financial constraints. However, it is not a perfect predictor \"\n",
    "    \"because other factors (like personal motivation, academic ability, or family expectations) also affect schooling.\"\n",
    ")\n",
    "\n",
    "print(conceptual_iv_predictor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49371ca4",
   "metadata": {},
   "source": [
    "b) [10] For the exclusion restriction to be valid, living close to a 4-year college in oneâ€™s county\n",
    "must not affect earnings directly. In other words, schooling must be the only channel through\n",
    "which geographic proximity can affect oneâ€™s earnings. Do you think it is valid? Why or why\n",
    "not?\n",
    "\n",
    "Assuming that the exclusion restriction holds, let us estimate the IV regression equation using data.\n",
    "Card (1993) uses the Young Men Cohort of the 1966 National Longitudinal Survey. The data file\n",
    "is card.dta. Here are its key variables:\n",
    "â€¢ wage: earnings\n",
    "â€¢ educ: years of schooling\n",
    "â€¢ exper: potential work experience (age - educ - 6)\n",
    "2\n",
    "â€¢ black: an indicator for the black population\n",
    "â€¢ south: an indicator for living in a Southern state\n",
    "â€¢ married: an indicator for being married\n",
    "â€¢ smsa: an indicator for living in a metropolitan area of a Southern state\n",
    "â€¢ nearc4: an indicator for having a 4-year college in the same county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc3e7478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  nearc2  nearc4  educ   age  fatheduc  motheduc    weight  momdad14  sinmom14  step14  reg661  reg662  reg663  reg664  reg665  reg666  reg667  reg668  reg669  south66  black  smsa  south  smsa66   wage  enroll   KWW     IQ  married  libcrd14  exper     lwage  expersq   u_lwage  lwage_hat    u_educ\n",
      "0  2.0     0.0     0.0   7.0  29.0       NaN       NaN  158413.0       1.0       0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0    1.0   1.0    0.0     1.0  548.0     0.0  15.0    NaN      1.0       0.0   16.0  6.306275    256.0  0.198047   6.108228 -3.067539\n",
      "1  3.0     0.0     0.0  12.0  27.0       8.0       8.0  380166.0       1.0       0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0    0.0   1.0    0.0     1.0  481.0     0.0  35.0   93.0      1.0       1.0    9.0  6.175867     81.0 -0.211160   6.387027 -1.727562\n",
      "2  4.0     0.0     0.0  12.0  34.0      14.0      12.0  367470.0       1.0       0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0    0.0   1.0    0.0     1.0  721.0     0.0  42.0  103.0      1.0       1.0   16.0  6.580639    256.0  0.204719   6.375920  1.012975\n",
      "3  5.0     1.0     1.0  11.0  27.0      11.0      12.0  380166.0       1.0       0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0    0.0   1.0    0.0     1.0  250.0     0.0  25.0   88.0      1.0       1.0   10.0  5.521461    100.0 -0.967905   6.489366 -2.253536\n",
      "4  6.0     1.0     1.0  12.0  34.0       8.0       7.0  367470.0       1.0       0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0    0.0   1.0    0.0     1.0  729.0     0.0  34.0  108.0      1.0       0.0   16.0  6.591674    256.0  0.125173   6.466501  1.090437\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "\n",
    "# Load the Card (1993) data\n",
    "card = pd.read_stata(\"data/Card.dta\")\n",
    "print(card.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05e36bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage</th>\n",
       "      <th>educ</th>\n",
       "      <th>exper</th>\n",
       "      <th>black</th>\n",
       "      <th>south</th>\n",
       "      <th>married</th>\n",
       "      <th>smsa</th>\n",
       "      <th>nearc4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3003.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>577.282410</td>\n",
       "      <td>13.263455</td>\n",
       "      <td>8.856146</td>\n",
       "      <td>0.233555</td>\n",
       "      <td>0.403654</td>\n",
       "      <td>2.271395</td>\n",
       "      <td>0.712957</td>\n",
       "      <td>0.682060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>262.958313</td>\n",
       "      <td>2.676913</td>\n",
       "      <td>4.141672</td>\n",
       "      <td>0.423162</td>\n",
       "      <td>0.490711</td>\n",
       "      <td>2.066823</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.465753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>394.250000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>537.500000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>708.750000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2404.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              wage         educ        exper        black        south  \\\n",
       "count  3010.000000  3010.000000  3010.000000  3010.000000  3010.000000   \n",
       "mean    577.282410    13.263455     8.856146     0.233555     0.403654   \n",
       "std     262.958313     2.676913     4.141672     0.423162     0.490711   \n",
       "min     100.000000     1.000000     0.000000     0.000000     0.000000   \n",
       "25%     394.250000    12.000000     6.000000     0.000000     0.000000   \n",
       "50%     537.500000    13.000000     8.000000     0.000000     0.000000   \n",
       "75%     708.750000    16.000000    11.000000     0.000000     1.000000   \n",
       "max    2404.000000    18.000000    23.000000     1.000000     1.000000   \n",
       "\n",
       "           married         smsa       nearc4  \n",
       "count  3003.000000  3010.000000  3010.000000  \n",
       "mean      2.271395     0.712957     0.682060  \n",
       "std       2.066823     0.452457     0.465753  \n",
       "min       1.000000     0.000000     0.000000  \n",
       "25%       1.000000     0.000000     0.000000  \n",
       "50%       1.000000     1.000000     1.000000  \n",
       "75%       4.000000     1.000000     1.000000  \n",
       "max       6.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card[[\"wage\", \"educ\", \"exper\", \"black\", \"south\", \"married\", \"smsa\", \"nearc4\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73df19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log(wage) variable for later use, making sure to handle nonpositive wages as NaN\n",
    "card[\"log_wage\"] = np.where(card[\"wage\"] > 0, np.log(card[\"wage\"]), np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659c536",
   "metadata": {},
   "source": [
    "c) [10] Estimate the OLS and IV regression (2SLS) models, in which the dependent variable is\n",
    "log(wage), the endogenous variable is educ, instrumented by nearc4, and exogenous variables\n",
    "are exper, black, south, married, and smsa. Summarize the key variablesâ€™ coefficients and their\n",
    "robust standard errors following the table presented below (you donâ€™t need to report the\n",
    "coefficients of the control variables). Does the framework suffer the weak instrument problem?\n",
    "How do you interpret the estimation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "231723ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingDataError",
     "evalue": "exog contains inf or nans",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMissingDataError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m Z_iv = pd.concat([X_exog, Z], axis=\u001b[32m1\u001b[39m)               \u001b[38;5;66;03m# instruments + exogenous regressors (with constant)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Use correct arguments for IV2SLS; cov_type is NOT a valid keyword for .fit()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m iv2sls_model = \u001b[43mIV2SLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_endog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_iv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Display summary of OLS and IV (2SLS) for key vars\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOLS Regression Results (Robust SE):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/sandbox/regression/gmm.py:157\u001b[39m, in \u001b[36mIV2SLS.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    155\u001b[39m lfit.exog_hat_params = xhatparams\n\u001b[32m    156\u001b[39m lfit.exog_hat = xhat  \u001b[38;5;66;03m# TODO: do we want to store this, might be large\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28mself\u001b[39m._results_ols2nd = \u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxhat\u001b[49m\u001b[43m)\u001b[49m.fit()\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m RegressionResultsWrapper(lfit)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:921\u001b[39m, in \u001b[36mOLS.__init__\u001b[39m\u001b[34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    918\u001b[39m     msg = (\u001b[33m\"\u001b[39m\u001b[33mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    919\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mAn exception will be raised in the next version.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    920\u001b[39m     warnings.warn(msg, ValueWarning)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_keys:\n\u001b[32m    924\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_keys.remove(\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:746\u001b[39m, in \u001b[36mWLS.__init__\u001b[39m\u001b[34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    745\u001b[39m     weights = weights.squeeze()\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m nobs = \u001b[38;5;28mself\u001b[39m.exog.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    749\u001b[39m weights = \u001b[38;5;28mself\u001b[39m.weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:200\u001b[39m, in \u001b[36mRegressionModel.__init__\u001b[39m\u001b[34m(self, endog, exog, **kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28mself\u001b[39m.pinv_wexog: Float64Array | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28mself\u001b[39m._data_attr.extend([\u001b[33m'\u001b[39m\u001b[33mpinv_wexog\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwendog\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwexog\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/base/model.py:270\u001b[39m, in \u001b[36mLikelihoodModel.__init__\u001b[39m\u001b[34m(self, endog, exog, **kwargs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.initialize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/base/model.py:95\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, endog, exog, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m missing = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mmissing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     94\u001b[39m hasconst = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mhasconst\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                              \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.k_constant = \u001b[38;5;28mself\u001b[39m.data.k_constant\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.exog = \u001b[38;5;28mself\u001b[39m.data.exog\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/base/model.py:135\u001b[39m, in \u001b[36mModel._handle_data\u001b[39m\u001b[34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     data = \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/base/data.py:675\u001b[39m, in \u001b[36mhandle_data\u001b[39m\u001b[34m(endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m     exog = np.asarray(exog)\n\u001b[32m    674\u001b[39m klass = handle_data_class_factory(endog, exog)\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m             \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/base/data.py:88\u001b[39m, in \u001b[36mModelData.__init__\u001b[39m\u001b[34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.const_idx = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.k_constant = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mself\u001b[39m._check_integrity()\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m._cache = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/econometrics/venv/lib/python3.13/site-packages/statsmodels/base/data.py:134\u001b[39m, in \u001b[36mModelData._handle_constant\u001b[39m\u001b[34m(self, hasconst)\u001b[39m\n\u001b[32m    132\u001b[39m exog_max = np.max(\u001b[38;5;28mself\u001b[39m.exog, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(exog_max).all():\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MissingDataError(\u001b[33m'\u001b[39m\u001b[33mexog contains inf or nans\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    135\u001b[39m exog_min = np.min(\u001b[38;5;28mself\u001b[39m.exog, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    136\u001b[39m const_idx = np.where(exog_max == exog_min)[\u001b[32m0\u001b[39m].squeeze()\n",
      "\u001b[31mMissingDataError\u001b[39m: exog contains inf or nans"
     ]
    }
   ],
   "source": [
    "# OLS regression\n",
    "ols_formula = \"log_wage ~ educ + exper + black + south + married + smsa\"\n",
    "ols_model = smf.ols(formula=ols_formula, data=card).fit(cov_type='HC1')\n",
    "\n",
    "# First stage: regress educ on instrument and controls\n",
    "first_stage_formula = \"educ ~ nearc4 + exper + black + south + married + smsa\"\n",
    "first_stage = smf.ols(formula=first_stage_formula, data=card).fit(cov_type='HC1')\n",
    "\n",
    "# Add the fitted values to the card DataFrame (not necessary, but informative for diagnostics)\n",
    "card['educ_hat'] = first_stage.fittedvalues\n",
    "\n",
    "# IV regression using 2SLS with statsmodels\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "\n",
    "# Set up dependent, endogenous, exog, and instrument variables\n",
    "y = card[\"log_wage\"]\n",
    "# ENDOGENOUS variable(s) to be instrumented: 'educ'\n",
    "# EXOGENOUS variable(s): 'exper', 'black', 'south', 'married', 'smsa' (with constant)\n",
    "# INSTRUMENT(s): 'nearc4' plus all exogenous variables (with constant)\n",
    "\n",
    "X_exog = card[[\"exper\", \"black\", \"south\", \"married\", \"smsa\"]]\n",
    "X_exog = sm.add_constant(X_exog)                    # exogenous regressors with a constant\n",
    "X_endog = card[[\"educ\"]]                            # endogenous regressors\n",
    "Z = card[[\"nearc4\"]]\n",
    "Z_iv = pd.concat([X_exog, Z], axis=1)               # instruments + exogenous regressors (with constant)\n",
    "\n",
    "# Use correct arguments for IV2SLS; cov_type is NOT a valid keyword for .fit()\n",
    "iv2sls_model = IV2SLS(y, X_endog, Z_iv).fit()\n",
    "\n",
    "# Display summary of OLS and IV (2SLS) for key vars\n",
    "print(\"OLS Regression Results (Robust SE):\")\n",
    "print(ols_model.summary().tables[1].as_text())\n",
    "\n",
    "print(\"\\nFirst Stage F-stat on nearc4 (weak instrument test):\")\n",
    "print(first_stage.summary().tables[2].as_text())\n",
    "\n",
    "print(\"\\nIV Regression Results (2SLS):\")\n",
    "iv_results_table = iv2sls_model.summary().tables[1].as_text()\n",
    "print(iv_results_table)\n",
    "\n",
    "# For reporting: get key coefficients and robust SEs from both models\n",
    "print(\"\\nKey results summary:\")\n",
    "print(f\"OLS: coef(educ) = {ols_model.params['educ']:.4f}, robust SE = {ols_model.bse['educ']:.4f}\")\n",
    "print(f\"IV : coef(educ) = {iv2sls_model.params['educ']:.4f}, robust SE = {iv2sls_model.bse['educ']:.4f}\")\n",
    "print(\"\\nFirst-stage F-statistic for nearc4 (rule of thumb: < 10 is considered weak):\")\n",
    "print(f\"F-statistic: {first_stage.f_test('nearc4=0').fvalue[0][0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383de98",
   "metadata": {},
   "source": [
    "d) [10] Studies on the returns to schooling often suggest that the returns on schooling measured\n",
    "in OLS may be overstated because of positive correlation between earnings and ability (recall\n",
    "that if you measure the effect of schooling on earnings with binary treatment variable (ğ·),\n",
    "selection bias will be positive due to the correlation between earnings and ability). Is your\n",
    "empirical result in line with the expectation? If not, what could be the reasons for the\n",
    "discrepancy? For the full points, provide the reasons beyond the in-class discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40371e6b",
   "metadata": {},
   "source": [
    "Question 3 [Stock & Watson E12.2]: IV & Panel Data â€“ Violent Movie and Crimes [45 points]\n",
    "Does viewing a violent movie lead to violent behavior? If so, the incidence of violent crimes, such\n",
    "as assaults, should rise following the release of a violent movie that attracts many viewers.\n",
    "Alternatively, movie viewing may substitute for other activities (such as alcohol consumption) that\n",
    "lead to violent behavior, so that assaults should fall when more viewers are attracted to the cinema.\n",
    "The stata file Movies.dta contains data on the number of assaults and movie attendance for 516\n",
    "weekends from 1995 through 2004.1 A detailed description is given in Movies_Description.pdf\n",
    "file. The data set includes weekend U.S. attendance for strongly violent movies (such as Hannibal),\n",
    "mildly violent movies (such as Spider-Man), and nonviolent movies (such as Finding Nemo).The\n",
    "data set also includes a count of the number of assaults for the same weekend in a subset of counties\n",
    "in the United States. Finally, the data set includes indicators for year, month, whether the weekend\n",
    "is a holiday, and various measures of the weather."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359b4fa",
   "metadata": {},
   "source": [
    "a) [5] Regress the logarithm of the number of assaults [ln_assaults = ln(assaults)] on the year\n",
    "and month indicators. Is there evidence of seasonality in assaults? That is, do there tend to\n",
    "be more assaults in some months than others? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de59863",
   "metadata": {},
   "source": [
    "b) [5] Regress total movie attendance (attend = attend_v + attend_m + attend_n) on the year\n",
    "and month indicators. Is there evidence of seasonality in movie attendance? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a615917",
   "metadata": {},
   "source": [
    "c) [5] Regress ln_assaults on attend_v, attend_m, attend_n, the year and month indicators,\n",
    "and the weather and holiday control variables available in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e108723a",
   "metadata": {},
   "source": [
    "d) [5] Based on the regression in c), does viewing a strongly violent movie increase or decrease assaults? By how much? Is the estimated effect statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab6d12",
   "metadata": {},
   "source": [
    "e) Based on the regression in c), does attendance at strongly violent movies affect assaults\n",
    "differently than attendance at moderately violent movies? Differently than attendance at\n",
    "nonviolent movies?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23b51b",
   "metadata": {},
   "source": [
    "f) This part is based on the regression in c). A strongly violent blockbuster movie is\n",
    "released, and the weekendâ€™s attendance at strongly violent movies increases by 6 million;\n",
    "meanwhile, attendance falls by 2 million for moderately violent movies and by 1 million\n",
    "for nonviolent movies. What is the predicted effect on assaults? Construct a 95%\n",
    "confidence interval for the change in assaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c37ff",
   "metadata": {},
   "source": [
    "g) [10] It is difficult to control for all the variables that affect assaults and that might be\n",
    "correlated with movie attendance. For example, the effect of the weather on assaults and\n",
    "movie attendance is only crudely approximated by the weather variables in the data set.\n",
    "However, the data set does include a set of instrumentsâ€”pr_attend_v, pr_attend_m, and\n",
    "pr_ attend_nâ€”that are correlated with attendance but are (arguably) uncorrelated with\n",
    "weekend-specific factors (such as the weather) that affect both assaults and movie\n",
    "attendance. These instruments use historical attendance patterns, not information on a\n",
    "particular weekend, to predict a filmâ€™s attendance in a given weekend. For example, if a\n",
    "filmâ€™s attendance is high in the second week of its release, then this can be used to predict\n",
    "that its attendance was also high in the first week of its release. (The details of the\n",
    "construction of these instruments are available in the Dahl and DellaVigna paper\n",
    "referenced in footnote 5.) Run the regression from (b) (including year, month, holiday, and\n",
    "weather controls) but now using pr_attend_v, pr_attend_m, and pr_attend_n as\n",
    "instruments for attend_v, attend_m, and attend_n. Use this IV regression to answer (b)(i)â€“\n",
    "(b)(iii)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3503b1",
   "metadata": {},
   "source": [
    "h) [5] Based on your analysis, what do you conclude about the effect of violent movies on\n",
    "violent behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee099016",
   "metadata": {},
   "source": [
    "Question 4: Difference-in-Differences â€“ ACA Medicaid Expansion [45 points]\n",
    "In this exercise, you will measure the effect of the ACA Medicaid expansion on health\n",
    "insurance coverage again, but exploiting panel data structure this time. The data file for the\n",
    "exercise is acamedicaid2.dta. It contains annual data on uninsured rates and socioeconomic\n",
    "variables at the county level between 2010 and 2017. Here are the key variables from the data set:\n",
    "â€¢ pctui: uninsured rate\n",
    "â€¢ pctui: uninsured rate\n",
    "â€¢ tot_pop: total population (in 1,000)\n",
    "â€¢ tot_pop: total population (in 1,000)\n",
    "â€¢ black: share of blacks\n",
    "â€¢ black: share of blacks\n",
    "â€¢ hisp: share of Hispanics\n",
    "â€¢ hisp: share of Hispanics\n",
    "â€¢ children: share of 19-year-olds and younger\n",
    "â€¢ children: share of 19-year-olds and younger\n",
    "â€¢ elderly: share of 65-year-olds and older\n",
    "â€¢ elderly: share of 65-year-olds and older\n",
    "â€¢ unemployment: the unemployment rate\n",
    "â€¢ unemployment: the unemployment rate\n",
    "â€¢ pci: per capita income\n",
    "â€¢ pci: per capita income\n",
    "â€¢ exp: status of participating in the ACA Medicaid expansion\n",
    "â€¢ exp: status of participating in the ACA Medicaid expansion\n",
    "â€¢ year: year\n",
    "â€¢ year: year\n",
    "â€¢ statefips: state identifier\n",
    "â€¢ statefips: state identifier\n",
    "â€¢ countyfips: county identifier\n",
    "â€¢ countyfips: county identifier\n",
    "As discussed in the previous exercise, states could choose to participate in the expansion or not,\n",
    "and they also could participate in different years. Most states participated in 2014, but a few states\n",
    "participated in the expansion between 2014 and 2020. In this exercise, we will focus on measuring\n",
    "the effect of the expansion in 2014 (e.g., a homogeneous treatment timing). In other words, the\n",
    "states that opted into the expansion in 2014 are considered as treated, and the states that did not\n",
    "\n",
    "participate before 2020 are considered as untreated. Remaining states (i.e., the late-participants)\n",
    "are excluded. The table above lists the states in the three groups. The final data set is balanced as\n",
    "a small number of counties lacking data in some years are excluded.\n",
    "The main goal of this exercise is to estimate the effect of the ACA Medicaid expansion on\n",
    "uninsured rates at the county level using a differences-in-differences (DID) framework.\n",
    "Throughout the exercise, compute robust standard errors clustered by state, and use the population\n",
    "of non-elderly adults (tot_pop Ã— (1 âˆ’ children âˆ’ elderly)) as a weight when computing averages\n",
    "and estimating regression models. Put your regression outputs only in your log file.\n",
    "\n",
    "Treated: AR, AZ, CA, CO, CT, DC, DE, HI, IA, IL, KY, MA, MD, MI, MN, ND, NH, NJ, NM, NV, NY, OH, OR, RI, VT, WA, WV\n",
    "Untreated: AL, FL, GA, ID, KS, ME, MO, MS, NC, NE, OK, SC, SD, TN, TX, UT, VA, WI, WY\n",
    "Excluded: AK, IN, LA, MT, PA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a313bd",
   "metadata": {},
   "source": [
    "a. Letâ€™s first check the pre-trends of the outcome variable to assess the parallel trends\n",
    "assumption. Make a time-series plot of the annual means of pctui between 2010 and 2017 for\n",
    "the treatment and control groups. When computing means, use the weighted means weighted\n",
    "by the population of non-elderly adults. Does the plot exhibit parallel trends between the two\n",
    "groups in the pre-period?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdbafb",
   "metadata": {},
   "source": [
    "b. Use regression to estimate the DID parameter. Without using controls, estimate the\n",
    "parameters in the following specifications. Do they give you the same coefficients for the DID\n",
    "parameter (ğ›¿)? How do you interpret the coefficient?\n",
    "(1) [Classic DID] ğ‘Œ_{cst} = ğ›¼_{1} + ğ›½ * ğ‘ƒğ‘œğ‘ ğ‘¡_{t} + ğ›¾ * ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡_{s} + ğ›¿ * ğ‘ƒğ‘œğ‘ ğ‘¡_{t} * ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡_{s} + ğœ€_{cst}\n",
    "(2) [Two-way FE (state)] ğ‘Œ_{cst} = ğ›¼_{2} + ğœ†_{s} + ğœ‡_{t} + ğ›¿_{2} * ğ·_{cst} + ğ‘’_{cst}\n",
    "(3) [Two-way FE (county)] ğ‘Œ_{cst} = ğ›¼_{3} + ğœ™_{c} + ğœŒ_{t} + ğ›¿_{3} * ğ·_{cst} + ğœ–_{cst}\n",
    "where ğ‘Œ_{cst}: pctui of county ğ‘ in state ğ‘  in year ğ‘¡;\n",
    "ğ‘ƒğ‘œğ‘ ğ‘¡_{t} : an indicator for years â‰¥ 2014;\n",
    "ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡_{s}: an indicator for participation states (exp = 1);\n",
    "ğœ†_{s}: state-fixed effects;\n",
    "ğœ‡_{t}, ğœŒ_{t}: year fixed effects;\n",
    "ğœ™_{c} : county fixed effects;\n",
    "ğ·_{cst}: indicator whether county ğ‘ in state ğ‘  is treated in year ğ‘¡:\n",
    "ğœ€_{cst}, ğ‘’_{cst}, ğœ–_{cst}: county-level mean-zero random shocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc239534",
   "metadata": {},
   "source": [
    "c. In the next analysis, the following time-varying county-level covariates (ğ‘‹_{cst}) will be\n",
    "included in the DID models: log(tot_pop), black, hisp, children, elderly, unemployment, and\n",
    "pci. Are they â€œgoodâ€ control variables in the sense that they are not the outcomes of the\n",
    "treatment? In what cases will they be â€œbadâ€ controls?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d1b03",
   "metadata": {},
   "source": [
    "d. Estimate the models with the covariates included. Do the three specifications give you the\n",
    "same DID coefficients? Why or why not? Think about this question in terms of the regression-\n",
    "anatomy formula. Based on these results, what is your conclusion on the impact of the ACA\n",
    "Medicaid expansion on uninsured rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41650d6",
   "metadata": {},
   "source": [
    "e. Make an event study plot like Miller, Johnson, and Wherry (2021) by estimating the\n",
    "following model.\n",
    "\n",
    "&)%*\n",
    "ğ‘Œ \"#$ = ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡# Ã— I ğ›½(\n",
    "(+&)%)\n",
    "(,&)%'\n",
    "ğŸ(ğ‘¡ = ğ‘¦) + ğœ†# + ğœ‡$ + ğ‘‹!#$ + ğ‘’\"#$\n",
    "\n",
    "Does the plot support the parallel trends assumption? Do you see any dynamics in the treatment\n",
    "effect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ecafe",
   "metadata": {},
   "source": [
    "f. Check if your DID result is robust by including state-specific linear time trends\n",
    "Lâˆ‘ ğŸ(ğ‘¦ = ğ‘ ) â‹… ğ‘¡\n",
    "( N in your DID specification. What is your conclusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5793fd46",
   "metadata": {},
   "source": [
    "g. Check if your DID result is robust by estimating the triple difference model in which you\n",
    "interact the following three variables: ğ‘ƒğ‘œğ‘ ğ‘¡$, ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡#, and ğ¿ğ‘œğ‘¤-ğ¼ğ‘›ğ‘ğ‘œğ‘šğ‘’\". ğ¿ğ‘œğ‘¤-ğ¼ğ‘›ğ‘ğ‘œğ‘šğ‘’\" is an\n",
    "indicator for being a low-income county defined as a county with its pci in 2013 lower than\n",
    "the 2013 median pci. Medicaid is the public insurance program mainly for the low-income\n",
    "population, so its impact on insurance coverage is expected to be larger in counties with lower\n",
    "income levels. Thus, if the DID result is actually shaped by the treatment effect (rather than\n",
    "state-specific trends), the DID coefficients are expected to be different between low-income\n",
    "and high-income counties. What is your conclusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f93e12",
   "metadata": {},
   "source": [
    "Question 5: Regression Discontinuity â€“ US Senate Elections [45 points]\n",
    "This exercise will measure the incumbency advantage in the US Senate elections using a\n",
    "regression discontinuity (RD) design. The incumbency advantage is defined as an incumbentâ€™s\n",
    "advantage in an election over challengers. Identifying the incumbency advantage from election\n",
    "results is not straightforward. High-quality politicians are more likely to win an election in general,\n",
    "so it may be unclear whether the candidateâ€™s quality or incumbency allows the incumbent to be re-\n",
    "elected. Therefore, a simple comparison of the probability of being elected between incumbents\n",
    "and new candidates may overestimate the incumbency advantage.\n",
    "In this exercise, we focus on measuring the effect of being an incumbent party on the vote share.\n",
    "Lee (2008) shows that using an RD design can effectively measure the advantage of an incumbent\n",
    "party in the US House elections. He uses the vote margin of a party in the previous election as a\n",
    "running variable (the partyâ€™s vote share â€“ the opposite partyâ€™s vote share). In a two-party system\n",
    "like the one in the US, having a positive vote margin means the party won the election. Thus, the\n",
    "discontinuity in the running variable occurs at the zero-vote margin, and the treatment (being\n",
    "elected) is defined as having a positive vote margin.\n",
    "This exercise will focus on US Senate elections to measure the incumbency advantage. The dataset\n",
    "is CIT_2019_Cambridge_senate.dta from Cattaneo, Frandsen, and Titiunik (2015). It contains the\n",
    "results of the US Senate elections from 1914 to 2010. The running variable is demmv, which\n",
    "measures the vote margin of the Democratic party in election t. The primary outcome variable of\n",
    "interest is demvoteshfor2, Democratic vote share in election ğ‘¡ + 2. In US Senate elections, an\n",
    "incumbent is up for re-election for the same seat not in the next election (ğ‘¡ + 1) but in the election\n",
    "after the next one (ğ‘¡ + 2). Make sure to read Section 4 of Cattaneo, Frandsen, and Titiunik (2015)\n",
    "to get a good grasp of the overall discussion of the US Senate elections and how the incumbency\n",
    "advantage is defined in this setting.\n",
    "The following website, https://rdpackages.github.io/, provides Stata and R codes you can use for\n",
    "RD analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b32bf9",
   "metadata": {},
   "source": [
    "a. Which RD design (Sharp RD vs. Fuzzy RD) is suitable for this study?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc83526",
   "metadata": {},
   "source": [
    "b. Draw the plots of demvoteshfor2 on demmv using the following features (for RD plots, use the fourth polynomial order for the global estimation).\n",
    "i. A scatter plot using raw data. Include a vertical line for the cutoff.\n",
    "ii. A Mimicking Variance (MV) RD plot with equally-spaced (ES) bins.\n",
    "iii. An MV RD plot with quantile-spaced (QS) bins.\n",
    "iv. An Integrated Mean Squared Error (IMSE) RD plot with ES bins\n",
    "v. An IMSE RD plot with QS bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a3391",
   "metadata": {},
   "source": [
    "c. What can you learn from b) â€“ for example, by comparing the scatter plot and RD plots\n",
    "in general, IMSE and MV RD plots, and ES and QS bins?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179dfc34",
   "metadata": {},
   "source": [
    "d. Estimate the RD treatment effect using the traditional approach. Specifically, use the\n",
    "fourth-order polynomials for a global parametric estimation. Report the p-value and 95%\n",
    "confidence interval for the RD coefficient using the heteroskedasticity-robust standard\n",
    "error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bcee7",
   "metadata": {},
   "source": [
    "e. Estimate the RD treatment effect using the continuity-based approach. Specifically, use\n",
    "a triangular kernel function and linear polynomial for estimation. Compute the Mean\n",
    "Squared Error (MSE) optimal bandwidth. Report the p-values and 95% confidence\n",
    "intervals for the RD coefficient using conventional and robust bias-correction standard\n",
    "errors. How does your result with this approach differ from the result in d)? What do you\n",
    "think are the reasons for the discrepancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c94999",
   "metadata": {},
   "source": [
    "f. Estimate the RD treatment effect using the local randomization approach. First, select\n",
    "the window for local randomization using the following settings in the randomization-\n",
    "based tests of the sharp null hypothesis of no treatment effect for each of the covariates\n",
    "listed below.\n",
    "\n",
    "i. The test statistic: the difference-in-means\n",
    "ii. Covariates: State-level Democratic percentage of the vote in the past presidential election (predemvoteshlag1), state population (population), Democratic vote share\n",
    "in Senate election ğ‘¡ âˆ’ 1 (demvoteshlag1), Democratic vote share in Senate election ğ‘¡ âˆ’ 2 (demvoteshlag2), an indicator for Democratic victory in Senate election ğ‘¡ âˆ’ 1 (demwinprv1), an indicator for Democratic victory in Senate election ğ‘¡ âˆ’ 2 (demwinprv2), an indicator for an open seat in election ğ‘¡ (dopen), an indicator for\n",
    "midterm (non-presidential) election in election t (dmidterm), and an indicator for whether the president of the US at ğ‘¡ is Democratic (dpresdem)\n",
    "\n",
    "iii. Randomization mechanism: complete (or fixed-margins) randomization\n",
    "iv. Significance level (Î±âˆ—) = 0.15\n",
    "v. Conduct simulated-based tests with 10,000 draws, rather than exact tests, when\n",
    "computing the p-values. See Chapter 2.2.1 in Cattaneo, Idorobo, and Titiunik\n",
    "(2018) for more information about a simulated-based test.\n",
    "\n",
    "Make a graph like Figure 1 in Cattaneo, Frandsen, and Titiunik (2015), which plots\n",
    "minimum p-values against %\n",
    "& (window length) for the window widths (i.e., ğ‘¥ values)\n",
    "between 0 and 10 with an increment of 0.125. What is your final choice for the window?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9597a6",
   "metadata": {},
   "source": [
    "g. Report the estimate of the RD effect from the local randomization approach with the\n",
    "settings specified in f). Report the p-value and 95% confidence interval based on 10,000\n",
    "simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a348c",
   "metadata": {},
   "source": [
    "h. Conduct the following two falsification tests for the RD design (focusing on the continuity-based approach with non-parametric local-linear estimation).\n",
    "\n",
    "i. Make an MV RD plot of demvoteshlag2 against the running variable, demmv, using QS bins. demvoteshlag2 is predetermined, so it is not expected to exhibit a discontinuity at the cutoff. Do you see any jump at the cutoff in your RD plot?\n",
    "\n",
    "ii. Construct 95% robust bias-correction confidence intervals for the RD effect (demvoteshfor2) using the following placebo cutoffs: âˆ’6, âˆ’4, âˆ’2, 2, 4, and 6.\n",
    " Do you see any significant effects at the placebo cutoffs? Based on these results, what is your conclusion regarding the validation of the RD design?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174e7a7",
   "metadata": {},
   "source": [
    "i. Based on your results from the three approaches, what is your conclusion regarding the existence/magnitude of the incumbency advantage in US Senate elections?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
